import asyncio
import aiohttp
import logging
import os
from collections import OrderedDict
import re
import time

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


# è¯»å–è®¢é˜…æ–‡ä»¶ä¸­çš„ URL
def read_subscribe_file(file_path):
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return [line.strip() for line in f if line.strip()]
    except FileNotFoundError:
        logging.error(f"æœªæ‰¾åˆ°è®¢é˜…æ–‡ä»¶: {file_path}")
        return []


# è¯»å–åŒ…å«æƒ³ä¿ç•™çš„ç»„åæˆ–é¢‘é“çš„æ–‡ä»¶
def read_include_list_file(file_path):
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            include_list = []
            current_group = None
            for line in f:
                line = line.strip()
                if line.startswith('group:'):
                    current_group = line.replace('group:', '')
                elif line:
                    if current_group:
                        include_list.append((current_group, line))
                    else:
                        include_list.append((None, line))
            return include_list
    except FileNotFoundError:
        logging.error(f"æœªæ‰¾åˆ°åŒ…å«åˆ—è¡¨æ–‡ä»¶: {file_path}")
        return []


# å¼‚æ­¥è·å– URL å†…å®¹å¹¶æµ‹è¯•å“åº”æ—¶é—´
async def fetch_url(session, url):
    start_time = time.time()
    try:
        async with session.get(url, timeout=10) as response:
            if response.status == 200:
                content = await response.text()
                elapsed_time = time.time() - start_time
                return content, elapsed_time
            else:
                logging.warning(f"è¯·æ±‚ {url} å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}")
    except Exception as e:
        logging.error(f"è¯·æ±‚ {url} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    return None, float('inf')


# è§£æ M3U æ ¼å¼å†…å®¹
def parse_m3u_content(content):
    channels = []
    lines = content.splitlines()
    i = 0
    while i < len(lines):
        line = lines[i].strip()
        if line.startswith('#EXTINF:'):
            info = line.split(',', 1)
            if len(info) == 2:
                metadata = info[0]
                name = info[1]
                tvg_id = re.search(r'tvg-id="([^"]+)"', metadata)
                tvg_name = re.search(r'tvg-name="([^"]+)"', metadata)
                group_title = re.search(r'group-title="([^"]+)"', metadata)
                i += 1
                if i < len(lines):
                    url = lines[i].strip()
                    channel = {
                        'name': name,
                        'url': url,
                        'tvg_id': tvg_id.group(1) if tvg_id else None,
                        'tvg_name': tvg_name.group(1) if tvg_name else None,
                        'group_title': group_title.group(1) if group_title else None,
                        'response_time': float('inf')
                    }
                    channels.append(channel)
        i += 1
    return channels


# è§£æ TXT æ ¼å¼å†…å®¹
def parse_txt_content(content):
    channels = []
    current_group = None
    lines = content.splitlines()
    for line in lines:
        line = line.strip()
        if line.endswith('#genre#'):
            current_group = line.replace('#genre#', '').strip()
        elif line:
            parts = line.split(',', 1)
            if len(parts) == 2:
                name, url = parts
                channel = {
                    'name': name,
                    'url': url,
                    'tvg_id': None,
                    'tvg_name': None,
                    'group_title': current_group,
                    'response_time': float('inf')
                }
                channels.append(channel)
    return channels


# åˆå¹¶å¹¶å»é‡é¢‘é“
def merge_and_deduplicate(channels_list):
    all_channels = []
    for channels in channels_list:
        all_channels.extend(channels)
    unique_channels = []
    url_set = set()
    for channel in all_channels:
        if channel['url'] not in url_set:
            unique_channels.append(channel)
            url_set.add(channel['url'])
    return unique_channels


# æµ‹è¯•æ¯ä¸ªé¢‘é“çš„å“åº”æ—¶é—´
async def test_channel_response_time(session, channel):
    start_time = time.time()
    try:
        async with session.get(channel['url'], timeout=10) as response:
            if response.status == 200:
                elapsed_time = time.time() - start_time
                channel['response_time'] = elapsed_time
    except Exception as e:
        logging.error(f"æµ‹è¯• {channel['url']} å“åº”æ—¶é—´æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    return channel


# è¿‡æ»¤å‡ºåŒ…å«åœ¨ include_list ä¸­çš„é¢‘é“
def filter_channels(channels, include_list):
    filtered_channels = []
    for group_title, name in include_list:
        for channel in channels:
            if (group_title is None or channel.get('group_title') == group_title) and channel['name'] == name:
                filtered_channels.append(channel)
    return filtered_channels


# ç”Ÿæˆ M3U æ–‡ä»¶ï¼Œå¢åŠ  EPG å›æ”¾æ”¯æŒ
def generate_m3u_file(channels, output_path, replay_days=7, custom_sort_order=None):
    # æŒ‰åˆ†ç»„æ ‡é¢˜åˆ†ç»„
    group_channels = {}
    for channel in channels:
        group_title = channel['group_title'] or ''
        if group_title not in group_channels:
            group_channels[group_title] = []
        group_channels[group_title].append(channel)

    def custom_sort_key(group_title):
        if custom_sort_order and group_title in custom_sort_order:
            return custom_sort_order.index(group_title)
        return float('inf')

    sorted_groups = sorted(group_channels.keys(), key=custom_sort_key)

    with open(output_path, 'w', encoding='utf-8') as f:
        f.write('#EXTM3U\n')
        for group_title in sorted_groups:
            group = group_channels[group_title]
            # ç»„å†…æŒ‰å“åº”æ—¶é—´æ’åº
            sorted_group = sorted(group, key=lambda x: x['response_time'])
            if group_title:
                f.write(f'#EXTGRP:{group_title}\n')
            for channel in sorted_group:
                metadata = '#EXTINF:-1'
                if channel['tvg_id']:
                    metadata += f' tvg-id="{channel["tvg_id"]}"'
                if channel['tvg_name']:
                    metadata += f' tvg-name="{channel["tvg_name"]}"'
                if channel['group_title']:
                    # å»é™¤ group_title ä¸­çš„å¤šä½™é€—å·
                    clean_group_title = channel["group_title"].strip(',').strip()
                    metadata += f' group-title="{clean_group_title}"'
                # æ·»åŠ å›æ”¾å‚æ•°
                replay_url = f'{channel["url"]}&replay=1&days={replay_days}'
                f.write(f'{metadata},{channel["name"]}\n')
                f.write(f'{replay_url}\n')
            f.write('\n')


# ç”Ÿæˆ TXT æ–‡ä»¶
def generate_txt_file(channels, output_path, custom_sort_order=None):
    # æŒ‰åˆ†ç»„æ ‡é¢˜åˆ†ç»„
    group_channels = {}
    for channel in channels:
        group_title = channel['group_title'] or ''
        if group_title not in group_channels:
            group_channels[group_title] = []
        group_channels[group_title].append(channel)

    def custom_sort_key(group_title):
        if custom_sort_order and group_title in custom_sort_order:
            return custom_sort_order.index(group_title)
        return float('inf')

    sorted_groups = sorted(group_channels.keys(), key=custom_sort_key)

    with open(output_path, 'w', encoding='utf-8') as f:
        for group_title in sorted_groups:
            group = group_channels[group_title]
            # ç»„å†…æŒ‰å“åº”æ—¶é—´æ’åº
            sorted_group = sorted(group, key=lambda x: x['response_time'])
            if group_title:
                f.write(f'{group_title}#genre#\n')
            for channel in sorted_group:
                f.write(f'{channel["name"]},{channel["url"]}\n')
            f.write('\n')


async def main():
    subscribe_file = 'config/subscribe.txt'
    output_m3u = 'output/result.m3u'
    output_txt = 'output/result.txt'
    # åŒ…å«æƒ³ä¿ç•™çš„ç»„åæˆ–é¢‘é“çš„æ–‡ä»¶
    include_list_file = 'config/include_list.txt'

    # è‡ªå®šä¹‰æ’åºé¡ºåº
    custom_sort_order = ['ğŸ„å¹¿ä¸œé¢‘é“', 'ğŸ“å¤®è§†é¢‘é“', 'ğŸ§å«è§†é¢‘é“', 'ğŸ¦„ï¸æ¸¯Â·æ¾³Â·å°', 'ğŸ…±AKTV', 'ç›´æ’­']

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = os.path.dirname(output_m3u)
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # è¯»å–è®¢é˜…æ–‡ä»¶
    urls = read_subscribe_file(subscribe_file)
    if not urls:
        logging.error("è®¢é˜…æ–‡ä»¶ä¸­æ²¡æœ‰æœ‰æ•ˆçš„ URLã€‚")
        return

    # è¯»å–åŒ…å«åˆ—è¡¨æ–‡ä»¶
    include_list = read_include_list_file(include_list_file)

    # å¼‚æ­¥è·å–æ‰€æœ‰ URL çš„å†…å®¹
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_url(session, url) for url in urls]
        results = await asyncio.gather(*tasks)

    all_channels = []
    for content, _ in results:
        if content:
            if '#EXTM3U' in content:
                channels = parse_m3u_content(content)
            else:
                channels = parse_txt_content(content)
            all_channels.append(channels)

    # åˆå¹¶å¹¶å»é‡é¢‘é“
    unique_channels = merge_and_deduplicate(all_channels)

    # æµ‹è¯•æ¯ä¸ªé¢‘é“çš„å“åº”æ—¶é—´
    async with aiohttp.ClientSession() as session:
        tasks = [test_channel_response_time(session, channel) for channel in unique_channels]
        unique_channels = await asyncio.gather(*tasks)

    # è¿‡æ»¤é¢‘é“
    filtered_channels = filter_channels(unique_channels, include_list)

    # ç”Ÿæˆ M3U å’Œ TXT æ–‡ä»¶
    generate_m3u_file(filtered_channels, output_m3u, custom_sort_order=custom_sort_order)
    generate_txt_file(filtered_channels, output_txt, custom_sort_order=custom_sort_order)

    logging.info("æˆåŠŸç”Ÿæˆ M3U å’Œ TXT æ–‡ä»¶ã€‚")


if __name__ == '__main__':
    asyncio.run(main())
